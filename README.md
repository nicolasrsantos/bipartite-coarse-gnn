This repository is the official PyTorch implementation of "Bipartite Graph Coarsening For Text Classification Using Graph Neural Networks" (CIARP 2023)

## Requirements

This code was implemented using Python 3.11.2, CUDA 11.8 and the following packages:

- `pytorch==2.0.1`
- `torch-geometric==2.3.0`
- `numpy==1.24.1`
- `networkx==3.0`
- `scikit-learn=1.2.2`
- `igraph==0.10.4`
- `scipy==1.7.2`
- `pyyaml==6.0`

## How to run the code

In order to run our method, you must perform the steps described below.

### Obtaining GloVe's embeddings

Since our method uses GloVe's embeddings, you **must** create a directory called *embeddings* in the project's root directory. Then, download the embeddings (glove.6B.zip) from this url https://nlp.stanford.edu/projects/glove/ and extract the 300d file (glove.6B.300d.txt) to the directory you created.

### Build graph

To build a graph you can simply run the following command:

    $ python build_graph.py --dataset <dataset_name>

After running this command, a new directory called *graphs* will be created in the *data* directory. Inside the *graphs* directory you will find many files, including the edge index and the masks required by torch geometric, a file containg the documents' classes (.y), the embeddings for each word (.x_word) and document (.x_doc), and a map used by our method.

This repository contain a few datasets available in the *data* directory, including the datasets we used in the experiments we ran.

### Coarsening

After building a graph, you can coarsen it using the MFBN framework, which is available inside the *mfbn* directory. In order to do so, you **must** first run the following command to generate the .ncol file required by the mfbn.

    $ python build_coarse_ncol.py --dataset <dataset_name>

This command will generate a .ncol file and move it to the *input* directory inside the *mfbn_coarsening* folder. After this step, you have to copy the x_doc and x_word files to the same directory. Then, you can finally coarsen the graph by running:

    $ python mfbn.py -cnf <config_file>

An example of the configuration file is available in mfbn's input folder. The most important parameters you must modify are the vertices, reduction_factor, and max_levels. Vertices is used to specify the number of nodes in each partition of the graph, reduction_factor controls the amount (in %) of contraction applied to each partition, and max_levels controls the number of contractions applied to each partition. For example, if max_levels = [10, 10], each partition will be coarsened hierarchically 10 times. For more information on mfbn's config file, check https://github.com/alanvalejo/mfbn

After coarsening the input graph, move the files generated by mfbn from the output directory to the data/graphs/ directory. Then, run the command below to parse the output files and adequate them to the input expected by torch geometric.

    $ python parse_coarse.py --dataset <dataset_name> --max_level <mfbn_max_level>

### GNN

Finally, you can train the GNN to perform text classification by running the following command:

    $ python train.py --dataset <dataset_name> --out_dim <number_of_classes>

The following arguments allow the modification of the GNN's hyperparameters:

- `--lr`

    Modifies the model's learning rate.
  
    Default: `1e-3`

- `--batch_size`

    Controls our method's batch size.
  
    Default: `32`

- `--hidden_dim`

    Number of dimensions used on GraphSAGE. 

    Default: `256`

- `--n_epochs`

    Number of training epochs.

    Default: `200`

- `--patience`

    Number of epochs without validation loss before early stopping.

    Default: `10`

- `--epoch_log`

    Prints information about the network's training every <epoch_log> steps.

    Default: `10`

- `--gpu`

    Trains the network using a GPU (if available).

    Default: `true`

- `--cpu`

    Trains the network using the CPU.

    Default: `false`

## Reference

This section will be updated once the paper is available on Springer.

## Acknowledgements

The Multilevel framework for bipartite networks (MFBN) plays an important role in our method. We adapted its original implementation to coarsen a graph using the cosine similarity of the words present in the datasets we used. For more information on MFBN, you can check the following repository: https://github.com/alanvalejo/mfbn.
